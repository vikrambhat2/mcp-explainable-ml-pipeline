# MCP Explainable ML Pipeline

This repository implements a diabetes risk prediction and explainability chatbot using a FastMCP server, LangGraph with a ReAct agent, Ollama for local LLM inference, and Streamlit for an interactive interface.

---

## âœ¨ Features

* **Diabetes Risk Prediction**: Predicts diabetes risk based on input features (e.g., age, BMI, pedigree).
* **Prediction Explanation (SHAP)**: Explains model predictions with SHAP values, showing feature contributions.
* **FastMCP Integration**: Tools exposed using FastMCPâ€™s server-client protocol.
* **LangGraph ReAct Agent**: Dynamically invokes prediction and explanation tools via Ollama (Qwen3:32b).
* **Streamlit Interface**: Enables user interaction with predictions, explanations, and visual insights.
* **Claude/LLM Interop**: Tools can also be tested with Claude or other MCP-compatible LLMs.

---

## ğŸ§° Prerequisites

* Python 3.8+
* Ollama (with `qwen3:32b` or `llama-3.3-70b-versatile` model pulled)
* Streamlit
* LangGraph, LangChain, Ollama, FastMCP (in `requirements.txt`)

---

## âš™ï¸ Installation

```bash
git clone https://github.com/vikrambhat2/mcp-explainable-ml-pipeline.git
cd mcp-explainable-ml-pipeline
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ§  Ollama Setup

```bash
ollama pull qwen3:32b
ollama serve
```

---

## ğŸ“ Project Structure

```plaintext
mcp-explainable-ml-pipeline/
â”œâ”€â”€ client.ipynb          # LLM-driven prediction+explanation testing via MCP
â”œâ”€â”€ client.py             # Streamlit chatbot UI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pima_diabetes.csv # Source data
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model.pkl         # Trained model (RandomForest)
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ predict.py        # Diabetes risk prediction tool
â”‚   â”œâ”€â”€ explain.py        # SHAP-based explanation tool
â”‚   â””â”€â”€ train_model.py    # Model training logic
â”œâ”€â”€ server.py             # FastMCP server exposing tools
â”œâ”€â”€ requirements.txt      # Dependency list
â””â”€â”€ README.md             # This file
```

---

## ğŸš€ Usage

### 1. Start the FastMCP Server

```bash
python server.py
```

> Ensure it's available at `http://localhost:8000/mcp`.

### 2. Launch the Streamlit Chatbot

```bash
streamlit run client.py
```

> Open [http://localhost:8501](http://localhost:8501) in your browser.

### 3. Example Queries

* `Predict diabetes risk for age 52, BMI 30.1, pedigree 0.52`
* `Explain prediction for age 52, BMI 30.1, pedigree 0.52`

The LLM will dynamically select between prediction and explanation tools.

---

## âš™ï¸ Configuration

Update `client.py` or LangGraph config to:

* Change MCP server URL
* Switch LLM (e.g., Claude vs Ollama)

---

## ğŸ” Tool Demonstrations

### âœ… Claude Desktop

Tool calls can be tested with Claude or any MCP-compatible client:

> `explain_diabetes_risk(age=52, bmi=30.1, diabetes_pedigree_function=0.52)`

<img width="1992" height="1596" alt="Claude demo" src="https://github.com/user-attachments/assets/cd367d43-bd3f-4565-b0d7-b82789fa49ee" />

---

### âœ… Streamlit Chat App

Live interaction and explanations via LLM agent:

<img width="3414" height="1874" alt="Streamlit chat demo" src="https://github.com/user-attachments/assets/738314fb-3433-479d-aa08-a128eade8878" />

---

## ğŸ§ª Explanation Tool Overview

The `explain_diabetes_risk` tool (in `tools/explain.py`) uses SHAP to return per-feature contributions to the model's prediction. This enables interpretability for each user-specific input.

**Example output:**

```json
{
  "age": 0.018,
  "bmi": 0.246,
  "diabetes_pedigree_function": 0.035
}
```

---

## ğŸ¤ Contributing

Fork the repo â†’ Create a branch â†’ Make changes â†’ Submit PR.

